#모 행복도에 따른 아동 행복 계산
#따릉이 전에 했던 플젝 주제 였으나 큰 관계성을 찾지 못해 엎어짐..ㅜ

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

fig, axs = plt.subplots(figsize=(16,8),ncols=4, nrows=2)
lm_features = ['모_결혼만족', '모_부부갈등', '모_우울']
for i, feature in enumerate(lm_features):
    row = int(i/4)
    col = i%4
    sns.regplot(x=feature, y='아동행복총합', data = X, ax=axs[row][col])

plt.scatter(X['모_결혼만족'], X['아동행복총합'], alpha= 0.2)
plt.xlabel('결혼만족도')
plt.ylabel('아동행복')

sns.lineplot(X['행복감'], X['아동행복총합'], data = X)

sns.pairplot(X, kind='reg')

plt.figure(figsize=(5, 5))
corr = X.corr()
sns.heatmap(corr, cmap='RdBu')

plt.figure(figsize=(9, 9))
corr = X.corr()
sns.heatmap(corr, cmap='RdBu')

from scipy.stats import pearsonr
X2.columns = ['아동행복총합', '학교적응', '결혼만족', '우울', '행복감']
X2

w17=pd.read_csv('data/w10_2017_data_221013.csv', encoding='cp949')
w=w17[['N_ID','JCh17shs006','JCh17shs007','JCh17shs008','JCh17shs009','JCh17shs010','JCh17shs011'
,'EMt17mrs001'   # '모' 배우자 만족도
,'EMt17mrs002'
,'EMt17mrs003'
,'EMt17mrs004'
,'EMt17prs012'   # '모' 일상적 스트레스
,'EMt17shs012'   # '모' 삶의 만족도
,'EMt17hlt049'   # '모' 주관적 건강상태
,'EMt17shs001'   # '모' 행복감
,'EMt17shs002' 
,'EMt17shs003'
,'EMt17shs004'
,'EMt17drn002'   # '모' 음주여부 및 횟수
,'EMt17drn003'   # '모' 음주량
,'EMt17drn004'   # '모' 과음 횟수
,'EMt17smk005'   # '모' 흡연 여부 및 양
]]

w=w.replace(' ',None)
w=w.dropna()
w=w.astype('int')
# w.describe()
for i in w.columns:
        w[i]= w[i].replace(88888888,3)
        w[i]= w[i].replace(99999999,3)
w['아동행복총합']= w[['JCh17shs006','JCh17shs007','JCh17shs008','JCh17shs009','JCh17shs010','JCh17shs011']].sum(axis=1)
w['모_배우자만족도']=w[['EMt17mrs001'   # '모' 배우자 만족도
,'EMt17mrs002'
,'EMt17mrs003'
,'EMt17mrs004']].sum(axis=1)
w['모_일상적스트레스']=w[['EMt17prs012']].sum(axis=1)
w['모_삶의만족도']=w[['EMt17shs012']].sum(axis=1)
w['모_주관적건강상태']=w[['EMt17hlt049']].sum(axis=1)
w['모_행복감']=w[['EMt17shs001'   # '모' 행복감
,'EMt17shs002' 
,'EMt17shs003'
,'EMt17shs004']].sum(axis=1)
w['모_음주여부및횟수']=w[['EMt17drn002']].sum(axis=1)
w['모_음주량']=w[['EMt17drn003']].sum(axis=1)
w['모_과음횟수']=w[['EMt17drn004']].sum(axis=1)
w['모_흡연여부및양']=w[['EMt17smk005']].sum(axis=1)

X = w[['아동행복총합','모_배우자만족도','모_일상적스트레스','모_삶의만족도','모_주관적건강상태','모_행복감', '모_음주여부및횟수', '모_음주량'
      , '모_과음횟수', '모_흡연여부및양']]
X.reset_index(inplace=True, drop = True)
sns.pairplot(X, kind='reg')

pearsonr(X['모_배우자만족도'], X['모_행복감'])[0]


w19= pd.read_csv('data_아동/w12_2019_data(공개용)_220719 (1).csv', encoding='cp949')

w = w19[[
'JCh19dsc031',
'JCh19dsc032',
'JCh19dsc033',
'JCh19dsc034',
'JCh19dsc035',
'JCh19dsc036',
'JCh19dsc037',
'JCh19dsc038',
    
'JCh19pss016',
    

# w12 생성. “아동” 부모애착: 
'JCh19ach025', 
'JCh19ach026',
'JCh19ach027',
'JCh19ach028',
'JCh19ach029',
'JCh19ach030',

'JCh19shs000', #전반적행복감


'JCh19sel023', #자아탄력성
'JCh19sel024',#자아정체감

'JCh19sfs000', #삶의만족도
'JCh19aut000', #아동자율성
'JCh19bod000', #신체적자아상
'JCh19str000', #학업스트레스



'LCh19acs000', #학업수행능력


#-----------------------------------
'ECh19mid018',
'ECh19mid019',
'ECh19mid020',
'ECh19mid021',
'ECh19mid022',
'ECh19mid023',
'ECh19mid024',
'ECh19mid025',
'ECh19mid026',
'ECh19mid027',
'ECh19mid028',
'ECh19mid029',
'ECh19mid030',
'ECh19mid031',   
'ECh19mid032'   ,
'ECh19eat026'   ,
'ECh19eat027'   ,
'ECh19eat028'   ,
'ECh19eat029'   ,
'ECh19eat030'   ,
'ECh19eat031'   ,
'ECh19eat032'   ,
'ECh19eat033'   ,
'ECh19eat034'   ,
'ECh19eat035'   ,
'EMt19mrs001'   ,
'EMt19mrs002'   ,
'EMt19mrs003'   ,
'EMt19mrs004'   ,
'EMt19prs012'   ,
'EMt19shs012'   ,
'EMt19hlt049'   ,
'EMt19dpr013'   ,
'EMt19dpr014'   ,
'EMt19dpr015'   ,
'EMt19dpr016'   ,
'EMt19dpr017'   ,
'EMt19dpr018',
'EMt19shs001'   ,
'EMt19shs002'   ,
'EMt19shs003'   ,
'EMt19shs004'   ,
'EMt19drn002'   ,
'EMt19drn003'   ,
'EMt19drn004'   ,
'EMt19smk005',
    
# ----------------------
    #부_결혼만족도
'FFt19mrs001'   ,
'FFt19mrs002'   ,
'FFt19mrs003'   ,
'FFt19mrs004'   ,
    
    #부 일상적스트레스
'FFt19prs012'   ,
    #부 삶의만족도
'FFt19shs012'   ,
    #부 주관적 건강상태
'FFt19hlt049'   ,
    
    #부 우울
'FFt19dpr013'   ,
'FFt19dpr014'   ,
'FFt19dpr015'   ,
'FFt19dpr016'   ,
'FFt19dpr017'   ,
'FFt19dpr018'   ,
    #부 주관적행복감
'FFt19shs001'   ,
'FFt19shs002'   ,
'FFt19shs003'   ,
'FFt19shs004'   ,
    

# -------------------------------
'DCh19dmg001',
'DMt19wfb001a',
'DMt19wfb002a',
'DMt19wfb003a',
'DMt19wfb004a',
#     w12 생성. “모” 일과 가정: 이점
# w12 생성. “모” 일과 가정: 갈등
# w12 생성. “모” 일과 양육: 이점
# w12 생성. “모” 일과 양육: 갈등

    'DCh19mid017', #아동 미디어 이용시간


'DCh19par001',
'DCh19par002'   ,
'DCh19par003',
'DCh19par004'   ,
'DCh19par005'   ,
'DCh19par006'   ,
'DCh19par007'   ,
'DCh19par008'   ,
'DCh19par009'   ,
'DCh19par010'   ,
'DCh19par011'   ,
'DCh19par012'   ,
'DCh19par013'   ,
'DCh19par014'   ,
'DCh19par015'   ,
'DCh19par016'   ,
'DCh19par017', #학습에 대한 부모 참여
    
    #혼자있는시간
'DCh19dsc015',
    
'DMt19crs052', #아동 부모감동
'DMt19crs053',
'DMt19crs054',
'DMt19crs055',
    



'ACh19amt101',
'ACh19amt102',
'ACh19amt103',
'ACh19amt104',
'ACh19amt105',
'ACh19amt106',
'ACh19amt107',
'ACh19amt108',
'ACh19amt109',
'ACh19amt110',
'ACh19amt111',
'ACh19amt112',
'ACh19amt113',
'ACh19amt114',
'ACh19amt115',
'ACh19amt116',
#     "아동" AMT T점수: 학업적 자기효능감
# "아동" AMT T점수: 자신감
# "아동" AMT T점수: 자기조절 효능감
# "아동" AMT T점수: 과제 수준 선호
# "아동" AMT T점수: 학업적 실패내성
# "아동" AMT T점수: 감정
# "아동" AMT T점수: 행동
# "아동" AMT T점수: 과제 난이도 선호
# "아동" AMT 백분위: 학업적 자기효능감
# "아동" AMT 백분위: 자신감
# "아동" AMT 백분위: 자기조절 효능감
# "아동" AMT 백분위: 과제 수준 선호
# "아동" AMT 백분위: 학업적 실패내성
# "아동" AMT 백분위: 감정
# "아동" AMT 백분위: 행동
# "아동" AMT 백분위: 과제 난이도 선호


'ACh19neo204',
'ACh19neo205',
'ACh19neo206',
'ACh19neo207',
'ACh19neo208',
'ACh19neo209',
'ACh19neo210',
'ACh19neo211',
'ACh19neo212',
'ACh19neo213',
'ACh19neo214',
'ACh19neo215',
'ACh19neo216',
'ACh19neo217',
'ACh19neo218',
    
#     "아동" NEO T점수: E외향성
# "아동" NEO T점수: E1사회성
# "아동" NEO T점수:  E2지배성
# "아동" NEO T점수: E3자극추구
# "아동" NEO T점수:  O개방성
# "아동" NEO T점수: O1창의성
# "아동" NEO T점수:  O2정서성
# "아동" NEO T점수:  O3사고유연성
# "아동" NEO T점수:  A친화성
# "아동" NEO T점수:  A1온정성
# "아동" NEO T점수:  A2신뢰성
# "아동" NEO T점수: A3관용성
# "아동" NEO T점수:  C성실성
# "아동" NEO T점수:  C1유능감
# "아동" NEO T점수:  C2조직성
# "아동" NEO T점수: C3책임감

]]

w1 = w
for i in w1.columns:
    w1[i] = w1[i].replace(' ',0)
    w1[i]= w1[i].replace(88888888,0)
    w1[i]= w1[i].replace(99999999,0)
    w1[i]= w1[i].replace(9,0)
w1= w1.astype('float')

for i in w1.columns:
    w1[i] = w1[i].replace(0,w1[i].mean())

len(w[['JCh19dsc031','JCh19dsc032','JCh19dsc033','JCh19dsc034',
        'JCh19dsc035', 'JCh19dsc036','JCh19dsc037','JCh19dsc038']].columns)

www['부_주관적행복감']=w1[[  #부 주관적행복감
'FFt19shs001'   ,
'FFt19shs002'   ,
'FFt19shs003'   ,
'FFt19shs004'   ,]].sum(axis=1)/len(w1[[  #부 주관적행복감
'FFt19shs001'   ,
'FFt19shs002'   ,
'FFt19shs003'   ,
'FFt19shs004'   ,]].columns)

www

www[['부_일상적스트레스','부_삶의만족도','부_주관적건강상태']] = w1[[  #부 일상적스트레스
'FFt19prs012'   ,
    #부 삶의만족도
'FFt19shs012'   ,
    #부 주관적 건강상태
'FFt19hlt049'   ,
        ]] 

www['아동부모감독'].value_counts()

wx['전반적행복'] = w1['JCh19shs000']
wx

corr_df = www.corr()
s = corr_df.unstack()

df = pd.DataFrame(s[s<1].sort_values(ascending=False), columns=['corr'])
dff = df.style.background_gradient(cmap='viridis')
dff



www['시간사용만족']=w1[['JCh19dsc031','JCh19dsc032','JCh19dsc033','JCh19dsc034',
        'JCh19dsc035', 'JCh19dsc036','JCh19dsc037',
        'JCh19dsc038']].sum(axis=1)/len(w1[['JCh19dsc031','JCh19dsc032','JCh19dsc033','JCh19dsc034',
        'JCh19dsc035', 'JCh19dsc036','JCh19dsc037','JCh19dsc038']].columns)

www['미디어기기중독']=w1[['ECh19mid018',
'ECh19mid019',
'ECh19mid020',
'ECh19mid021',
'ECh19mid022',
'ECh19mid023',
'ECh19mid024',
'ECh19mid025',
'ECh19mid026',
'ECh19mid027',
'ECh19mid028',
'ECh19mid029',
'ECh19mid030',
'ECh19mid031',
'ECh19mid032',
]].sum(axis=1)/len(w1[['ECh19mid018',
'ECh19mid019',
'ECh19mid020',
'ECh19mid021',
'ECh19mid022',
'ECh19mid023',
'ECh19mid024',
'ECh19mid025',
'ECh19mid026',
'ECh19mid027',
'ECh19mid028',
'ECh19mid029',
'ECh19mid030',
'ECh19mid031',
'ECh19mid032']].columns)

www['모-식생활중독']=w1[[
'ECh19eat026'   ,
'ECh19eat027'   ,
'ECh19eat028'   ,
'ECh19eat029'   ,
'ECh19eat030'   ,
'ECh19eat031'   ,
'ECh19eat032'   ,
'ECh19eat033'   ,
'ECh19eat034'   ,
'ECh19eat035'   ,
]].sum(axis=1)/len(w1[[
'ECh19eat026'   ,
'ECh19eat027'   ,
'ECh19eat028'   ,
'ECh19eat029'   ,
'ECh19eat030'   ,
'ECh19eat031'   ,
'ECh19eat032'   ,
'ECh19eat033'   ,
'ECh19eat034'   ,
'ECh19eat035'   ,]].columns)

www['모-결혼만족도']=w1[[
# 모-결혼만족도
'EMt19mrs001'   ,
'EMt19mrs002'   ,
'EMt19mrs003'   ,
'EMt19mrs004'   ,
    ]].sum(axis=1)/len(w1[[
'EMt19mrs001'   ,
'EMt19mrs002'   ,
'EMt19mrs003'   ,
'EMt19mrs004'   ,]].columns)
 
www['모-일상적스트레스']=w1[[
#모-일상적스트레스
'EMt19prs012'   ,
        ]].sum(axis=1)/len(w1[[
'EMt19prs012'   ,]].columns)
    
'EMt19shs012'   ,

www['모-삶의만족도']=w1[[
#모-삶의만족도
'EMt19shs012'   ,
        ]].sum(axis=1)/len(w1[[
'EMt19shs012'   ,]].columns)
    

www['모-주관적건강상태']=w1[[
#모-주관적건강상태
'EMt19hlt049'   ,
            ]].sum(axis=1)/len(w1[[
'EMt19hlt049'   ,]].columns)
    
www['모-우울']=w1[[
#모-우울
'EMt19dpr013'   ,
'EMt19dpr014'   ,
'EMt19dpr015'   ,
'EMt19dpr016'   ,
'EMt19dpr017'   ,
'EMt19dpr018',
]].sum(axis=1)/len(w1[[
'EMt19dpr013'   ,
'EMt19dpr014'   ,
'EMt19dpr015'   ,
'EMt19dpr016'   ,
'EMt19dpr017'   ,
'EMt19dpr018',  
]].columns)

www['모-주관적행복감']=w1[[
#모-주관적행복감
'EMt19shs001'   ,
'EMt19shs002'   ,
'EMt19shs003'   ,
'EMt19shs004'   ,
]].sum(axis=1)/len(w1[[
'EMt19shs001'   ,
'EMt19shs002'   ,
'EMt19shs003'   ,
'EMt19shs004'   ,
]].columns)

www['모-음주여부횟수']=w1[[
#모-음주여부횟수
'EMt19drn002'   ,
]].sum(axis=1)/len(w1[[
'EMt19drn002'   ,
]].columns) 

www['모-음주량']=w1[[
#모-음주량
'EMt19drn003'   ,
    ]].sum(axis=1)/len(w1[[
'EMt19drn003'   ,
]].columns) 

www['모-과음횟수']=w1[[
#모-과음횟수
'EMt19drn004'   ,
    ]].sum(axis=1)/len(w1[[
'EMt19drn004'  ,
]].columns) 

www['모-흡연여부']=w1[[
#모-흡연여부
'EMt19smk005',
    ]].sum(axis=1)/len(w1[[
'EMt19smk005'  ,
]].columns) 

www.columns

from scipy.stats import pearsonr
for i in www.columns:
    if pearsonr(www['전반적행복감'], www[i])[0] < 1 and pearsonr(www['전반적행복감'], www[i])[0] > 0.6 :
        print(pearsonr(www['전반적행복감'], www[i])[0])
        print(i)

X= w1[[
'JCh19shs000',
'FFt19shs001'   ,
'FFt19shs002'   ,
'FFt19shs003'   ,
'FFt19shs004'   ,
'FFt19drn002'   ,
'FFt19drn003'   ,
'FFt19drn004',
'FFt19smk005',
'FFt19wfb003'   ,
'FFt19wfb004'   ,
'FFt19wfb005'   ,
'FFt19wfb006'   ,
'FFt19wfb007'   ,
'FFt19wfb008'   ,
'FFt19wfb009'   ,
'FFt19wfb010'   ,
'FFt19wfb011'   ,
'DMt19crs052',
'DMt19crs053',
'DMt19crs054',
'DMt19crs055',
'DCh19hlt049',
'DHu19cmm000',
'ACh19neo205',
'ACh19neo206',
'ACh19neo212',
'ACh19neo213',
'ACh19neo214',
'ACh19neo215',
'ACh19neo216',
'ACh19neo217']]

fig, axs = plt.subplots(figsize=(16,8),ncols=4, nrows=4)
lm_features = ['FFt19shs001'   ,
'DMt19crs052',
'DMt19crs053',
'DMt19crs054',
'DMt19crs055',
'DCh19hlt049',
'DHu19cmm000',
'ACh19neo205',
'ACh19neo206',
]
for i, feature in enumerate(lm_features):
    row = int(i/4)
    col = i%4
    sns.regplot(x=feature, y='JCh19shs000', data = X, ax=axs[row][col])

ww= w1[[
    'JCh19shs000',
'DMt19crs052',
'DMt19crs053',
'DMt19crs054',
'DMt19crs055',
'DCh19hlt049',
'DHu19cmm000',
'DHu19cmm009',
'DHu19cmm010',
'DHu19cmm021',
'DHu19cmm022',
'DHu19cmm023',
'DHu19cmm024',
'FFt19shs001'   ,
'FFt19shs002'   ,
'FFt19shs003'   ,
'FFt19shs004'  ,]]

plt.figure(figsize=(9, 9))
corr = test.corr()
sns.heatmap(corr, cmap='RdBu', annot=True)
plt.scatter(www['책임감'],www['전반적행복감'])

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.datasets import load_boston
import warnings
warnings.filterwarnings('ignore')

lm_features = ['학습부모참여', '전반적행복감', '학업수행능력', '아동부모감독',
               '부_삶의만족도', '부_주관적행복감','모-주관적행복감' ,'모-삶의만족도']
fig, axs = plt.subplots(figsize=(16,8), ncols = 4, nrows=2)
for i , feature in enumerate(lm_features): # 인덱스 값과 , 피쳐 값 추출
    row = int(i/4) #행
    col = i%4 #열
    sns.regplot(x=feature, y = '책임감', data=www, ax=axs[row][col])

from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures

# method는 표준 정규 분포 변환(Standard), 최대값/최소값 정규화(MinMax), 로그변환(Log) 결정
# p_degree는 다향식 특성을 추가할 때 적용. p_degree는 2이상 부여하지 않음. 
def get_scaled_data(method='None', p_degree=None, input_data=None):
    if method == 'Standard':
        scaled_data = StandardScaler().fit_transform(input_data)
    elif method == 'MinMax':
        scaled_data = MinMaxScaler().fit_transform(input_data)
    elif method == 'Log':
        scaled_data = np.log1p(input_data)
    else:
        scaled_data = input_data

    if p_degree != None:
        scaled_data = PolynomialFeatures(degree=p_degree, 
                                         include_bias=False).fit_transform(scaled_data)

    return scaled_data

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

df[df['사회성']>80]

#이상치제거
import numpy as np

def get_outlier(df=None, column=None, weight=1.5):
    # fraud에 해당하는 column 데이터만 추출, 1/4 분위와 3/4 분위 지점을 np.percentile로 구함. 
    fraud = df
    quantile_25 = np.percentile(fraud.values, 25)
    quantile_75 = np.percentile(fraud.values, 75)
    # IQR을 구하고, IQR에 1.5를 곱하여 최대값과 최소값 지점 구함. 
    iqr = quantile_75 - quantile_25
    iqr_weight = iqr * weight
    lowest_val = quantile_25 - iqr_weight
    highest_val = quantile_75 + iqr_weight
    # 최대값 보다 크거나, 최소값 보다 작은 값을 아웃라이어로 설정하고 DataFrame index 반환. 
    outlier_index = fraud[(fraud < lowest_val) | (fraud > highest_val)].index
    return outlier_index
    

outlier_index = get_outlier(df=df, column='책임감', weight=1.5)
print('이상치 데이터 인덱스:', outlier_index)

y = df['책임감']
X = df.drop(columns=['책임감'])
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=156)
lr = LinearRegression()
lr.fit(X_train, y_train)
pred = lr.predict(X_test)
mse = mean_squared_error(y_test, pred)
rmse = np.sqrt(mse)

print(f'MSE: {mse}, RMSE: {rmse}, R2: {r2_score(y_test, pred)}')
# rmse 는 price에서 rmse만큼 차이가 난다.
# R2는 1에 가까울 수록 좋은 것

pd.Series(np.round(lr.coef_,1), index=X.columns).sort_values(ascending=False) 

from sklearn.model_selection import cross_val_score

neg_mse_scores= cross_val_score(lr,X,y,scoring='neg_mean_squared_error',cv=5)

neg_mse_scores # 회귀는 점수가 음수 형태로 나옴

np.mean(np.sqrt(neg_mse_scores * -1))
# 양수로 변경하고 루트 씌워서 rmse로 만들기
# 5개로 나눴을 때 모델 점수 값의 평균을 구함

#릿지
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score
import warnings
warnings.filterwarnings('ignore')

ridge = Ridge(alpha=10)
neg_mse_scores = cross_val_score(ridge, X, y, scoring = 'neg_mean_squared_error', cv=5) 

np.mean(np.sqrt(neg_mse_scores * -1))
# 5.828658946215817 : alpha 값 조정 안했을때
# 값이 작을 수록 더 좋은 성능

alphas=[0,0.1,1,10,100]
for alpha in alphas:
    ridge = Ridge(alpha=alpha)
    neg_mse_scores = cross_val_score(ridge, X, y, scoring='neg_mean_squared_error', cv=5)
    print(np.mean(np.sqrt(neg_mse_scores * -1)))
    
# alpha가 100일때가 가장 좋은 점수

fig, axs = plt.subplots(figsize = (18,6), nrows = 1, ncols = 5)
coeff_df = pd.DataFrame()
for pos, alpha in enumerate(alphas):
    ridge = Ridge(alpha=alpha)
    ridge.fit(X,y)
    coeff = pd.Series(ridge.coef_, index=X.columns)
    colname = 'alpha:' +str(alpha)
    coeff_df[colname] = coeff
    coeff = coeff.sort_values(ascending=False)
    axs[pos].set_title(colname)
    axs[pos].set_xlim(-3,6)
    sns.barplot(x = coeff.values, y = coeff.index, ax = axs[pos])
plt.show

ridge_alphas = [0,0.1,1,10,100]
sort_columns = 'alpha:' + str(ridge_alphas[0])
coeff_df.sort_values(by=sort_columns, ascending=False)

# 랏쏘, 엘라스틱
from sklearn.linear_model import Lasso, ElasticNet

# alpha값에 따른 회귀 모델의 폴드 평균 RMSE를 출력하고 회귀 계수값들을 DataFrame으로 반환 
def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None, 
                        verbose=True, return_coeff=True):
    coeff_df = pd.DataFrame()
    if verbose : print('####### ', model_name , '#######')
    for param in params:
        if model_name =='Ridge': model = Ridge(alpha=param)
        elif model_name =='Lasso': model = Lasso(alpha=param)
        elif model_name =='ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7)
        neg_mse_scores = cross_val_score(model, X_data_n, 
                                             y_target_n, scoring="neg_mean_squared_error", cv = 5)
        avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))
        print('alpha {0}일 때 5 폴드 세트의 평균 RMSE: {1:.3f} '.format(param, avg_rmse))
        # cross_val_score는 evaluation metric만 반환하므로 모델을 다시 학습하여 회귀 계수 추출
        
        model.fit(X_data_n , y_target_n)
        if return_coeff:
            # alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame의 컬럼으로 추가. 
            coeff = pd.Series(data=model.coef_ , index=X_data_n.columns )
            colname='alpha:'+str(param)
            coeff_df[colname] = coeff
    
    return coeff_df
# end of get_linear_regre_eval

lasso_alphas = [0.05,0.07,0.1,0.5,1,3] 
#값이 너무 크면 릿지와 다르게 회귀계수(W)가 0이 되는 경우가 생김
coeff_lasso_df = get_linear_reg_eval('Lasso',params=lasso_alphas, X_data_n= X,y_target_n=y)

elastic_alphas = [0.05,0.07,0.1,0.5,1,3] 
coeff_elastic_df = get_linear_reg_eval('ElasticNet',params=elastic_alphas, X_data_n= X,y_target_n=y)

from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures

# method는 표준 정규 분포 변환(Standard), 최대값/최소값 정규화(MinMax), 로그변환(Log) 결정
# p_degree는 다향식 특성을 추가할 때 적용. p_degree는 2이상 부여하지 않음. 
def get_scaled_data(method='None', p_degree=None, input_data=None):
    if method == 'Standard':
        scaled_data = StandardScaler().fit_transform(input_data)
    elif method == 'MinMax':
        scaled_data = MinMaxScaler().fit_transform(input_data)
    elif method == 'Log':
        scaled_data = np.log1p(input_data)
    else:
        scaled_data = input_data

    if p_degree != None:
        scaled_data = PolynomialFeatures(degree=p_degree, 
                                         include_bias=False).fit_transform(scaled_data)

    return scaled_data

alphas = [0.1,1,10,100]
scale_methods = [(None, None),
                 ('Standard', None),
                 ('Standard',2),
                 ('MinMax', None),
                 ('MinMax',2),
                 ('Log', None)]
for scale_method in scale_methods:
    X_scaled_data = get_scaled_data(method=scale_method[0],p_degree=scale_method[1], input_data=X)
    print(f'{scale_method[0]}   {scale_method[1]}')
    get_linear_reg_eval('Ridge', params= alphas, X_data_n=X_scaled_data, y_target_n=y, return_coeff=False)
    
#로그가 정규분포 형태이므로 제일 좋은 값...이라고 한듯

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge, Lasso

log_y = np.log1p(y_test)
log_pred = np.log1p(pred)
squared_error = (log_y - log_pred)**2
rmsle = np.sqrt(np.mean(squared_error))

rmse=np.sqrt(mean_squared_error(y_test,pred))

from sklearn.metrics import mean_squared_error, mean_absolute_error

def rmsle(y,pred):
    log_y = np.log1p(y)
    log_pred = np.log1p(pred)
    squared_error = (log_y - log_pred)**2
    rmsle = np.sqrt(np.mean(squared_error))
    return rmsle

def rmse(y,pred):
    return np.sqrt(mean_squared_error(y,pred))

def evaluate_regr(y, pred):
    rmse= rmse(y,pred)
    mae= 'rmse'
    print(f' RMSE : {rmse}, MAE:{mae}')

y = df['책임감']
X = df.drop(columns='책임감')
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=156)

lr_reg = LinearRegression()
lr_reg.fit(X_train, y_train)
pred = lr_reg.predict(X_test)

print('rmsle',np.sqrt(np.mean(squared_error)))
print('rmse',np.sqrt(mean_squared_error(y_test,pred)))
print('mae',mean_absolute_error(y_test, pred))

def get_top_error_data(y_test,pred,n_top=5):
    result_df = pd.DataFrame(y_test.values, columns=['real'])
    result_df['pred']=np.round(pred)
    result_df['diff']= np.abs(result_df['real']- result_df['pred'])
    print(result_df.sort_values('diff', ascending=False)[:n_top])

get_top_error_data(y_test,pred,n_top=5)
df['사회성'].hist()
